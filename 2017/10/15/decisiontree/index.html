<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.9.0">

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="决策树算法是经常使用的数据挖掘算法，这是因为决策树就像一个人脑中的决策模型一样，呈现出来非常直观。基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。
决策树分类的应用场景非常广泛，在各行各业都有应用，比如在金融行业可以用决策树做贷款风险评估，医疗行业可以用决策树生成辅助">
    

    <!--Author-->
    
        <meta name="author" content="John Doe">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="利用决策树进行Titanic生存预测">
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Hexo">

    <!--Type page-->
    
        <meta property="og:type" content="article">
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary">
    

    <!-- Title -->
    
    <title>利用决策树进行Titanic生存预测 - Hexo</title>

    <!-- Tachyons Core CSS -->
    <link rel="stylesheet" href="https://unpkg.com/tachyons/css/tachyons.min.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<!-- Main Content -->
<!-- Banner -->
<!-- Banner -->
<div class="w-100 bg-1 ph5-ns ph3 text-light">
    
    <nav class="db dt-l w-100 mw8 center border-box pv3">
        <a class="db dtc-l v-mid link dim w-100 w-25-l tc tl-l mb2 mb0-l white" href="/" title="Hexo">
            <img src="http://www.codeblocq.com/assets/projects/hexo-theme-anodyne/assets/anodyne.svg" class="dib h3" alt="Hexo">
        </a>
        <div class="db dtc-l v-mid w-100 w-75-l tc tr-l">
            
                <a class="link dim f6 f5-l dib mr3 mr4-l white" 
                    href="/" 
                    title="Home">
                    Home
                </a>
            
                <a class="link dim f6 f5-l dib mr3 mr4-l white" 
                    href="/archives" 
                    title="Archives">
                    Archives
                </a>
            
                <a class="link dim f6 f5-l dib mr3 mr4-l white" 
                    href="/categories" 
                    title="Categories">
                    Categories
                </a>
            
                <a class="link dim f6 f5-l dib mr3 mr4-l white" 
                    href="/2019/05/30/resume/index.html" 
                    title="About">
                    About
                </a>
            
        </div>
    </nav>

    <!-- Title -->
    <div class="w-100 mw8 center vh-40 dt">
        <div class="dtc v-mid white">
            <h1 class="f1-l f2-m tc tc-m tl-ns">利用决策树进行Titanic生存预测</h1>
            <p class="f4 fw3 pab-100px tc tc-m tl-ns">2017-10-15</p>
        </div>
    </div>

    <!-- Icon -->
    <div class="relative w-100 mw8 center white dn dn-m db-ns">
        <i class="header-icon fa fa-file-text-o"></i>
    </div>
</div>

<!-- Content -->
<div class="w-100 ph2 ph4-m ph5-l mv5 mv6-l">
    <div class="content">
        <div class="mw8 center">
            <div class="cf">
                <div class="fl w-100 w-70-l mw7 left fw3 lh-copy pr4-ns pr0-m post-content">
                    <!-- Tags Vertical -->
                    

                    <!-- Main Post Content -->
                    <p>决策树算法是经常使用的数据挖掘算法，这是因为决策树就像一个人脑中的决策模型一样，呈现出来非常直观。基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。</p>
<p>决策树分类的应用场景非常广泛，在各行各业都有应用，比如在金融行业可以用决策树做贷款风险评估，医疗行业可以用决策树生成辅助诊断，电商行业可以用决策树对销售额进行预测等。</p>
<p>在了解决策树的原理后，今天我们用sklearn工具解决一个实际的问题：泰坦尼克号乘客的生存预测。</p>
<h1 id="sklearn中的决策树模型"><a href="#sklearn中的决策树模型" class="headerlink" title="sklearn中的决策树模型"></a>sklearn中的决策树模型</h1><p>首先，sklearn中自带的决策树分类器DecisionTreeClassifier，方法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf = DecisionTreeClassifier(criterion=&apos;entropy&apos;)</span><br></pre></td></tr></table></figure>

<p>到目前为止，sklearn中只实现了ID3与CART决策树，所以我们暂时只能使用这两种决策树，在构造DecisionTreeClassifier类时，其中有一个参数是criterion，意为标准。它决定了构造的分类树是采用ID3分类树，还是CART分类树，对应的取值分别是entropy或者gini：</p>
<ul>
<li>entropy: 基于信息熵，也就是ID3算法，实际结果与C4.5相差不大；</li>
<li>gini：默认参数，基于基尼系数。CART算法是基于基尼系数做属性划分的，所以criterion=gini时，实际上执行的是CART算法。</li>
</ul>
<p>我们通过设置criterion=’entropy’可以创建一个ID3决策树分类器，然后打印下clf，看下决策树在sklearn中是个什么东西？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DecisionTreeClassifier(class_weight=None, criterion=&apos;entropy&apos;, max_depth=None,</span><br><span class="line">            max_features=None, max_leaf_nodes=None,</span><br><span class="line">            min_impurity_decrease=0.0, min_impurity_split=None,</span><br><span class="line">            min_samples_leaf=1, min_samples_split=2,</span><br><span class="line">            min_weight_fraction_leaf=0.0, presort=False, random_state=None,</span><br><span class="line">            splitter=&apos;best&apos;)</span><br></pre></td></tr></table></figure>

<p>在构造决策树分类器后，我们可以使用fit方法让分类器进行拟合，使用predict方法对新数据进行预测，得到预测的分类结果，也可以使用score方法得到分类器的准确率。</p>
<p>下面这个表格是fit方法、predict方法和score方法的作用。</p>
<table>
<thead>
<tr>
<th align="center">方法表</th>
<th align="center">作用</th>
</tr>
</thead>
<tbody><tr>
<td align="center">fit(features,labels)</td>
<td align="center">通过特征矩阵，分类标识，让分类器进行拟合</td>
</tr>
<tr>
<td align="center">predict(features)</td>
<td align="center">返回预测结果</td>
</tr>
<tr>
<td align="center">score(features,labels)</td>
<td align="center">返回准确率</td>
</tr>
</tbody></table>
<h1 id="Titanic乘客生存预测"><a href="#Titanic乘客生存预测" class="headerlink" title="Titanic乘客生存预测"></a>Titanic乘客生存预测</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a><strong>问题描述</strong></h2><p>泰坦尼克海难是著名的十大灾难之一，究竟多少人遇难，各方统计的结果不一。</p>
<p>其中数据集格式为csv，一共有两个文件：</p>
<ul>
<li>train.csv是训练数据集，包含特征信息和存活与否的标签；</li>
<li>test.csv: 测试数据集，只包含特征信息。</li>
</ul>
<p>现在我们需要用决策树分类对训练集进行训练，针对测试集中的乘客进行生存预测，并告知分类器的准确率。</p>
<p>在训练集中，包括了以下字段，它们具体为：</p>
<table>
<thead>
<tr>
<th align="center">字段</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center">PassengerId</td>
<td align="center">乘客编号</td>
</tr>
<tr>
<td align="center">Survived</td>
<td align="center">是否幸存</td>
</tr>
<tr>
<td align="center">Pclass</td>
<td align="center">船票等级</td>
</tr>
<tr>
<td align="center">Name</td>
<td align="center">乘客姓名</td>
</tr>
<tr>
<td align="center">Sex</td>
<td align="center">乘客性别</td>
</tr>
<tr>
<td align="center">SibSp</td>
<td align="center">亲戚数量（兄妹、配偶数）</td>
</tr>
<tr>
<td align="center">Parch</td>
<td align="center">亲戚数量（父母、子女数）</td>
</tr>
<tr>
<td align="center">Ticket</td>
<td align="center">船票号码</td>
</tr>
<tr>
<td align="center">Fare</td>
<td align="center">船票价格</td>
</tr>
<tr>
<td align="center">Cabin</td>
<td align="center">船舱</td>
</tr>
<tr>
<td align="center">Embarked</td>
<td align="center">登陆港口</td>
</tr>
</tbody></table>
<h2 id="生存预测的关键流程"><a href="#生存预测的关键流程" class="headerlink" title="生存预测的关键流程"></a><strong>生存预测的关键流程</strong></h2><p>我们要对训练集中乘客的生存进行预测，这个过程可以划分为两个重要的阶段：</p>
<ol>
<li><strong>准备阶段</strong>：我们首先需要对训练集、测试集的数据进行探索，分析数据质量，并对数据进行清洗，然后通过特征选择对数据进行降维，方便后续分类运算；</li>
<li><strong>分类阶段</strong>：首先通过训练集的特征矩阵、分类结果得到决策树分类器，然后将分类器应用于测试集。然后我们对决策树分类器的准确性进行分析，并对决策树模型进行可视化。</li>
</ol>
<h3 id="模块1：数据探索"><a href="#模块1：数据探索" class="headerlink" title="模块1：数据探索"></a>模块1：数据探索</h3><p>数据探索这部分虽然对分类器没有实质作用，但是不可忽略。我们只有足够了解这些数据的特性，才能帮助我们做数据清洗、特征选择。</p>
<p>那么如何进行数据探索呢？这里有一些函数你需要了解：</p>
<ul>
<li>使用info()了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度；</li>
<li>使用describe()了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等；</li>
<li>使用describe(include=[‘O’])查看字符串类型（非数字）的整体情况；</li>
<li>使用head查看前几行数据（默认是前5行）；</li>
<li>使用tail查看后几行数据（默认是最后5行）。</li>
</ul>
<p>我们可以使用Pandas便捷地处理这些问题：</p>
<figure class="highlight plain"><figcaption><span>pandas as pd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"># 数据加载</span><br><span class="line">train_data = pd.read_csv(&apos;./Titanic_Data/train.csv&apos;)</span><br><span class="line">test_data = pd.read_csv(&apos;./Titanic_Data/test.csv&apos;)</span><br><span class="line"># 数据探索</span><br><span class="line">print(train_data.info())</span><br><span class="line">print(&apos;-&apos;*30)</span><br><span class="line">print(train_data.describe())</span><br><span class="line">print(&apos;-&apos;*30)</span><br><span class="line">print(train_data.describe(include=[&apos;O&apos;]))</span><br><span class="line">print(&apos;-&apos;*30)</span><br><span class="line">print(train_data.head())</span><br><span class="line">print(&apos;-&apos;*30)</span><br><span class="line">print(train_data.tail())</span><br></pre></td></tr></table></figure>

<p>运行结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">RangeIndex: 891 entries, 0 to 890</span><br><span class="line">Data columns (total 12 columns):</span><br><span class="line">PassengerId    891 non-null int64</span><br><span class="line">Survived       891 non-null int64</span><br><span class="line">Pclass         891 non-null int64</span><br><span class="line">Name           891 non-null object</span><br><span class="line">Sex            891 non-null object</span><br><span class="line">Age            714 non-null float64</span><br><span class="line">SibSp          891 non-null int64</span><br><span class="line">Parch          891 non-null int64</span><br><span class="line">Ticket         891 non-null object</span><br><span class="line">Fare           891 non-null float64</span><br><span class="line">Cabin          204 non-null object</span><br><span class="line">Embarked       889 non-null object</span><br><span class="line">dtypes: float64(2), int64(5), object(5)</span><br><span class="line">memory usage: 83.6+ KB</span><br><span class="line">None</span><br><span class="line">------------------------------</span><br><span class="line">       PassengerId    Survived     ...           Parch        Fare</span><br><span class="line">count   891.000000  891.000000     ...      891.000000  891.000000</span><br><span class="line">mean    446.000000    0.383838     ...        0.381594   32.204208</span><br><span class="line">std     257.353842    0.486592     ...        0.806057   49.693429</span><br><span class="line">min       1.000000    0.000000     ...        0.000000    0.000000</span><br><span class="line">25%     223.500000    0.000000     ...        0.000000    7.910400</span><br><span class="line">50%     446.000000    0.000000     ...        0.000000   14.454200</span><br><span class="line">75%     668.500000    1.000000     ...        0.000000   31.000000</span><br><span class="line">max     891.000000    1.000000     ...        6.000000  512.329200</span><br><span class="line"></span><br><span class="line">[8 rows x 7 columns]</span><br><span class="line">------------------------------</span><br><span class="line">                                          Name   Sex   ...       Cabin Embarked</span><br><span class="line">count                                      891   891   ...         204      889</span><br><span class="line">unique                                     891     2   ...         147        3</span><br><span class="line">top     Peter, Mrs. Catherine (Catherine Rizk)  male   ...     B96 B98        S</span><br><span class="line">freq                                         1   577   ...           4      644</span><br><span class="line"></span><br><span class="line">[4 rows x 5 columns]</span><br><span class="line">------------------------------</span><br><span class="line">   PassengerId  Survived  Pclass    ...        Fare Cabin  Embarked</span><br><span class="line">0            1         0       3    ...      7.2500   NaN         S</span><br><span class="line">1            2         1       1    ...     71.2833   C85         C</span><br><span class="line">2            3         1       3    ...      7.9250   NaN         S</span><br><span class="line">3            4         1       1    ...     53.1000  C123         S</span><br><span class="line">4            5         0       3    ...      8.0500   NaN         S</span><br><span class="line"></span><br><span class="line">[5 rows x 12 columns]</span><br><span class="line">------------------------------</span><br><span class="line">     PassengerId  Survived  Pclass    ...      Fare Cabin  Embarked</span><br><span class="line">886          887         0       2    ...     13.00   NaN         S</span><br><span class="line">887          888         1       1    ...     30.00   B42         S</span><br><span class="line">888          889         0       3    ...     23.45   NaN         S</span><br><span class="line">889          890         1       1    ...     30.00  C148         C</span><br><span class="line">890          891         0       3    ...      7.75   NaN         Q</span><br><span class="line"></span><br><span class="line">[5 rows x 12 columns]</span><br></pre></td></tr></table></figure>

<h3 id="模块2：数据清洗"><a href="#模块2：数据清洗" class="headerlink" title="模块2：数据清洗"></a>模块2：数据清洗</h3><p>通过数据探索，我们发现Age、Fare和Cabin这三个字段的数据有所缺失。其中Age为年龄字段，是数值型，我们可以通过平均值进行补齐；Fare为船票价格，是数值型，我们也可以通过其他人购买船票的平均值进行补齐。</p>
<p>代码实现如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 数据清洗</span><br><span class="line"># 使用平均年龄来填充年龄中的 nan 值</span><br><span class="line">train_data[&apos;Age&apos;].fillna(train_data[&apos;Age&apos;].mean(), inplace=True)</span><br><span class="line">test_data[&apos;Age&apos;].fillna(test_data[&apos;Age&apos;].mean(),inplace=True)</span><br><span class="line"># 使用票价的均值填充票价中的 nan 值</span><br><span class="line">train_data[&apos;Fare&apos;].fillna(train_data[&apos;Fare&apos;].mean(), inplace=True)</span><br><span class="line">test_data[&apos;Fare&apos;].fillna(test_data[&apos;Fare&apos;].mean(),inplace=True)</span><br><span class="line">print(train_data[&apos;Embarked&apos;].value_counts())</span><br><span class="line"></span><br><span class="line"># 使用登录最多的港口来填充登录港口的 nan 值</span><br><span class="line">train_data[&apos;Embarked&apos;].fillna(&apos;S&apos;, inplace=True)</span><br><span class="line">test_data[&apos;Embarked&apos;].fillna(&apos;S&apos;,inplace=True)</span><br></pre></td></tr></table></figure>

<h3 id="模块3：特征选择"><a href="#模块3：特征选择" class="headerlink" title="模块3：特征选择"></a>模块3：特征选择</h3><p>特征选择是分类器的关键。特征选择不同，得到的分类器也不同。那么我们该选择哪些特征做生存的预测呢？</p>
<p>通过数据探索我们发现，PassengerId为乘客编号，对分类没有作用，可以放弃；Name为乘客姓名，对分类没有作用，可以放弃；Cabin字段缺失值太多，可以放弃；Ticket字段为船票号码，杂乱无章且无规律，可以放弃。其余的字段包括：Pclass、Sex、Age、SibSp、Parch和Fare，这些属性分别表示了乘客的船票等级、性别、年龄、亲戚数量以及船票价格，可能会和乘客的生存预测分类有关系。具体是什么关系，我们可以交给分类器来处理。</p>
<p>因此我们先将Pclass、Sex、Age等这些其余的字段作特征，放到特征向量features里。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 特征选择</span><br><span class="line">features = [&apos;Pclass&apos;, &apos;Sex&apos;, &apos;Age&apos;, &apos;SibSp&apos;, &apos;Parch&apos;, &apos;Fare&apos;, &apos;Embarked&apos;]</span><br><span class="line">train_features = train_data[features]</span><br><span class="line">train_labels = train_data[&apos;Survived&apos;]</span><br><span class="line">test_features = test_data[features]</span><br></pre></td></tr></table></figure>

<p>特征值里有一些是字符串，这样不方便后续的运算，需要转成数值类型，比如Sex字段，有male和female两种取值。我们可以把它变成Sex=male和Sex=female两个字段，数值用0或1来表示。</p>
<p>同理Embarked有S、C、Q三种可能，我们也可以改成Embarked=S、Embarked=C和Embarked=Q三个字段，数值用0或1来表示。</p>
<p>那该如何操作呢，我们可以使用sklearn特征选择中的DictVectorizer类，用它将可以处理符号化的对象，将符号转成数字0/1进行表示。具体方法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dvec=DictVectorizer(sparse=False)</span><br><span class="line">train_features=dvec.fit_transform(train_features.to_dict(orient=&apos;record&apos;))</span><br><span class="line">print(dvec.feature_names_)</span><br></pre></td></tr></table></figure>

<p>可以看到代码中使用了fit_transform这个函数，它可以将特征向量转化为特征值矩阵。然后我们看下dvec在转化后的特征属性是怎样的，即查看dvec的feature_names_属性值，方法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(dvec.feature_names_)</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;Age&apos;, &apos;Embarked=C&apos;, &apos;Embarked=Q&apos;, &apos;Embarked=S&apos;, &apos;Fare&apos;, &apos;Parch&apos;, &apos;Pclass&apos;, &apos;Sex=female&apos;, &apos;Sex=male&apos;, &apos;SibSp&apos;]</span><br></pre></td></tr></table></figure>

<p>可以看到原本是一列的Embarked，变成了“Embarked=C”“Embarked=Q”“Embarked=S”三列。Sex列变成了“Sex=female”“Sex=male”两列。</p>
<p>这样train_features特征矩阵就包括10个特征值（列），以及891个样本（行），即891行，10列的特征矩阵。</p>
<h3 id="模块4：决策树模型"><a href="#模块4：决策树模型" class="headerlink" title="模块4：决策树模型"></a>模块4：决策树模型</h3><p>现在使用ID3算法，即在创建DecisionTreeClassifier时，设置criterion=‘entropy’，然后使用fit进行训练，将特征值矩阵和分类标识结果作为参数传入，得到决策树分类器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line"># 构造ID3决策树</span><br><span class="line">clf = DecisionTreeClassifier(criterion=&apos;entropy&apos;)</span><br><span class="line"># 决策树训练</span><br><span class="line">clf.fit(train_features, train_labels)</span><br></pre></td></tr></table></figure>

<h3 id="模块5：模型预测-amp-评估"><a href="#模块5：模型预测-amp-评估" class="headerlink" title="模块5：模型预测&amp;评估"></a>模块5：模型预测&amp;评估</h3><p>在预测中，我们首先需要得到测试集的特征值矩阵，然后使用训练好的决策树clf进行预测，得到预测结果pred_labels：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_features=dvec.transform(test_features.to_dict(orient=&apos;record&apos;))</span><br><span class="line"># 决策树预测</span><br><span class="line">pred_labels = clf.predict(test_features)</span><br></pre></td></tr></table></figure>

<p>在模型评估中，决策树提供了score函数可以直接得到准确率，但是我们并不知道真实的预测结果，所以无法用预测值和真实的预测结果做比较。我们只能使用训练集中的数据进行模型评估，可以使用决策树自带的score函数计算下得到的结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 得到决策树准确率</span><br><span class="line">acc_decision_tree = round(clf.score(train_features, train_labels), 6)</span><br><span class="line">print(u&apos;score准确率为 %.4lf&apos; % acc_decision_tree)</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">score准确率为 0.9820</span><br></pre></td></tr></table></figure>

<p>你会发现你刚用训练集做训练，再用训练集自身做准确率评估自然会很高。但这样得出的准确率并不能代表决策树分类器的准确率。</p>
<p>这是为什么呢？</p>
<p>因为我们没有测试集的实际结果，因此无法用测试集的预测结果与实际结果做对比。如果我们使用score函数对训练集的准确率进行统计，正确率会接近于100%（如上结果为98.2%），无法对分类器的在实际环境下做准确率的评估。</p>
<p>那么有什么办法，来统计决策树分类器的准确率呢？</p>
<p>这里可以使用K折交叉验证的方式，交叉验证是一种常用的验证分类准确率的方法，原理是拿出大部分样本进行训练，少量的用于分类器的验证。K折交叉验证，就是做K次交叉验证，每次选取K分之一的数据作为验证，其余作为训练。轮流K次，取平均值。</p>
<p>K折交叉验证的原理是这样的：</p>
<ol>
<li>将数据集平均分割成K个等份；</li>
<li>使用1份数据作为测试数据，其余作为训练数据；</li>
<li>计算测试准确率；</li>
<li>使用不同的测试集，重复2、3步骤。</li>
</ol>
<p>在sklearn的model_selection模型选择中提供了cross_val_score函数。cross_val_score函数中的参数cv代表对原始数据划分成多少份，也就是我们的K值，一般建议K值取10，因此我们可以设置CV=10，我们可以对比下score和cross_val_score两种函数的正确率的评估结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line"># 使用K折交叉验证 统计决策树准确率</span><br><span class="line">print(u&apos;cross_val_score准确率为 %.4lf&apos; % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_val_score准确率为 0.7835</span><br></pre></td></tr></table></figure>

<p>可以看到，score函数的准确率为0.9820，cross_val_score准确率为 0.7835。</p>
<p>这里很明显，对于不知道测试集实际结果的，要使用K折交叉验证才能知道模型的准确率。</p>
<h3 id="模块6：决策树可视化"><a href="#模块6：决策树可视化" class="headerlink" title="模块6：决策树可视化"></a>模块6：决策树可视化</h3><p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import tree</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;PATH&quot;] += os.pathsep + &apos;C:/softwares/graphviz/bin&apos;</span><br><span class="line">import graphviz</span><br><span class="line">dot_data = tree.export_graphviz(clf,out_file=None)</span><br><span class="line"></span><br><span class="line">graph  = graphviz.Source(dot_data)</span><br><span class="line">graph.view()</span><br></pre></td></tr></table></figure>

<h2 id="决策树模型使用技巧总结"><a href="#决策树模型使用技巧总结" class="headerlink" title="决策树模型使用技巧总结"></a>决策树模型使用技巧总结</h2><p>我用泰坦尼克乘客生存预测案例把决策树模型的流程跑了一遍。总结实战过程，有以下几点需要注意：</p>
<ol>
<li>特征选择是分类模型好坏的关键。选择什么样的特征，以及对应的特征值矩阵，决定了分类模型的好坏。通常情况下，特征值不都是数值类型，可以使用DictVectorizer类进行转化；</li>
<li>模型准确率需要考虑是否有测试集的实际结果可以做对比，当测试集没有真实结果可以对比时，需要使用K折交叉验证cross_val_score；</li>
<li>Graphviz可视化工具可以很方便地将决策模型呈现出来，帮助你更好理解决策树的构建。</li>
</ol>

                    
                    <!-- Tags Bottom -->
                    

                    <!-- Comments -->
                    



                </div>
                <div class="fl w-100 w-30-l center fw3 lh-copy pl4-ns tl black-50">
                    
                    <hr class="dn-l mw4 black-50 mt5" />
                    
                    <!-- Widget 1: About -->
                    <div class="mt5 mt0-l">
    <article class="dt db-l mw8 mw8-m mw5-ns center ml0-l bg-white mv3">
        <div class="dn dtc-m db-l v-mid tc pr4 pr0-l" style="min-width: 6rem;">
            <img src="http://tachyons.io/img/avatar_1.jpg" class="mb4-l br-100 h3 w3 h4-l w4-l dib" title="John Doe">
        </div>
        <div class="dtc db-l v-mid lh-copy measure center f6 black-50 tj">
            My name is Alvin Qin and this is my blog.<br>I am a data mining  engineer with a strong front-end focus.<br>
        </div>
    </article>
</div>

                    <hr class="dn-l mw4 black-50 mt5" />
                    
                    <!-- Widget 2: Categories -->
                    

                    <!-- Widget 3: Recent Posts -->
                    <div class="mt5 tc tl-l">
    <h3>Recent Posts</h3>
    
        <p>
            <a href="/2019/08/02/recommend/">Untitled</a>
        </p>
    
        <p>
            <a href="/2019/05/30/resume/">resume</a>
        </p>
    
        <p>
            <a href="/2019/01/20/plot2/">利用Python进行数据可视化（2）</a>
        </p>
    
        <p>
            <a href="/2018/12/27/plot1/">利用Python进行数据可视化（1）</a>
        </p>
    
        <p>
            <a href="/2018/11/15/linear/">线性回归的正则化</a>
        </p>
    
</div>
                </div>
            </div>
        </div>
    </div>
</div>


<!-- Footer -->
<div class="bg-1 ph2 ph5-ns pv5">
        <div class="mv8">
            <div class="center tc">
                
                    <div class="dib mh3">
                        <a class="f3 f2-ns white dim" href="https://twitter.com/?lang=en" target="_blank">
                            <i class="fa fa-twitter"></i>
                        </a>
                    </div>
                
                    <div class="dib mh3">
                        <a class="f3 f2-ns white dim" href="https://www.facebook.com/" target="_blank">
                            <i class="fa fa-facebook"></i>
                        </a>
                    </div>
                
                    <div class="dib mh3">
                        <a class="f3 f2-ns white dim" href="https://dribbble.com/" target="_blank">
                            <i class="fa fa-dribbble"></i>
                        </a>
                    </div>
                
                    <div class="dib mh3">
                        <a class="f3 f2-ns white dim" href="https://github.com/klugjo/hexo-theme-anodyne" target="_blank">
                            <i class="fa fa-github"></i>
                        </a>
                    </div>
                
                    <div class="dib mh3">
                        <a class="f3 f2-ns white dim" href="https://plus.google.com/" target="_blank">
                            <i class="fa fa-google-plus"></i>
                        </a>
                    </div>
                
                    <div class="dib mh3">
                        <a class="f3 f2-ns white dim" href="https://www.behance.net/" target="_blank">
                            <i class="fa fa-behance"></i>
                        </a>
                    </div>
                
                    <div class="dib mh3">
                        <a class="f3 f2-ns white dim" href="https://500px.com/" target="_blank">
                            <i class="fa fa-500px"></i>
                        </a>
                    </div>
                
                    <div class="dib mh3">
                        <a class="f3 f2-ns white dim" href="mailto:test@example.com" target="_blank">
                            <i class="fa fa-envelope"></i>
                        </a>
                    </div>
                
                    <div class="dib mh3">
                        <a class="f3 f2-ns white dim" href="/#" target="_blank">
                            <i class="fa fa-rss"></i>
                        </a>
                    </div>
                
            </div>
            <div class="f6 f5-ns center tc white pt5 fw3">
                @Untitled. All right reserved | Design & Hexo <a class="link dim white" href="http://www.codeblocq.com/">Jonathan Klughertz</a>
            </div>
        </div>
    </div>

<!-- After Footer -->
<!-- Disqus Comments -->



</body>

</html>